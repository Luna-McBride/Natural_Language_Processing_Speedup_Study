<h1 align="center">
<b>Natural Language Processing Speedup Study</b>
</h1>

![Speed](https://github.com/Luna-McBride/Natural_Language_Processing_Speedup_Study/blob/main/Speed.png)

Tools Used: PyTorch, PyTorch Lightning, Cython, LoRA, PEFT, CUDA, Numba

Models Used: A Basic Transformer Architecture built in PyTorch, Bart, Flan T5

This analysis exists in two parts across two files. The first takes an assignment for my Natural Language Processing class (CSCI-5832) that was not well-designed for a weaker computer like mine and stretches what speedup can be done with it to its limits. The second one is the final project for the same class, which discusses GPU usage on Google Colab using Numba's CUDA JIT decorator to send the training and generation functions to the GPU for speedy processing.

The reason for the separate discussions is to highlight the importance of speedup as a way to reduce the barrier to entry of these kinds of models. The project that uses Google Cloud costs money to use the GPU with the memory level needed to work with the Flan T5 Large. I spent $10 dollars to do so, but not everyone can spare that much to do so. This is also why the assignment variant specifically works to not change operating systems. Linux may be cool to have on the system, but there are too many people out there who cannot risk breaking their computers to get it running. I could have used CU's supercomputers at any point to get around this restriction, but not everyone has access to this either. We, as computing professionals, need to consider what kind of walls we are putting up in front of the people who could very well be the coders of the future, but do not have the money to work with the tools they percieve as being necessary due to processing power. The assignment not only proves that the expensive tools are not necessary to create great models, but provides more of an entrypoint for people who are scared of the pricepoint.

---

Luna_McBride_HW4: Originally assignment 4 for my Natural Language Processing class. This assignment looks at a translation dataset from English to Spanish. My computer could not handle the notebook as intended, so it became a study of speedup and memory management based on my Laptop. I wanted to see how far I could stretch my laptop (a Lenovo dual-core with only 8 GB of RAM) on this assignment, so changing operating systems was out of the question. I could not use GPU operations because my laptop has an Intel GPU, and Intel's GPU libraries are currently Linux only. The options used for speedup based on these rules includes: 

- Set unions for fast array flattening

- Numpy vectorization for quicker list operations

- PyTorch as_tensor() instead of tensor() for tensor conversion to minimize copying

- A Cython wrapper around many areas of code to send the Python code to the underlying C compiler. This was done instead of PyTorch.compile since that is Linux only and the point in this portion was to see how far my laptop could be stretched. This not only sped up the code, but made the cells with it more memory efficient.

- A PyTorch Lightning wrapper around both models. This helped the first model so much that it took training down to around 6 minutes for 10 epochs. For reference, before this was added, 2 epochs could not even finish overnight. It also helped manage memory for both models, with overload of RAM mitigated by writes to disk.

- Gradient Accumulation in Bart. A batch size of 2 with a gradient accumulation of 32 may be the same action as a batch size of 64, but it was way quicker with a lower memory threshold to meet.

- PEFT + LoRA actually made it reasonable to do any training of the Bart model on my laptop through rank-based parameter selection to train the model. This still took a while to finish and did not provide as robust results as a full model training would, but it could actually train on my laptop and provide some good results by leveraging Bart's previous training (Bleu of 32 versus the other model's Bleu of 13.)

---

NLP_Project: The project done for the class at the end of the semester. This looked more at Numba GPU usage for speed on Google Colab. It explores Few-Shot Learning for business flows using Flan T5's model. There are only a few lines of data to work with made by manually converting flows created for businesses into a text form (it is a project for natural language processing, after all) then testing both traditional training methods and the approach shown for sentiment analysis here (https://www.graphcore.ai/posts/flan-t5-sweet-results-with-the-smaller-more-efficient-llm). The input conversation data was reverse-generated by ChatGPT from the models to have a conversation. A Bart summarization model was also tested against for robustness when comparing full conversations and summaries when building the business flows (via this fine-tuned Bart conversation summarization model: https://huggingface.co/Mr-Vicky-01/Bart-Finetuned-conversational-summarization). The major speedup was caused by:

- Cython, once again used for its compilation power on the summarization model.

- Numba, whose JIT decorator sent the training and text generation of the Flan T5 model to the A100 GPU via CUDA. This was not only quick to implement, but also showed how fast the GPUs can really be. For reference, the final test generation finished in a couple minutes when attempts to use the CPU did not even finish in an hour and a half.
